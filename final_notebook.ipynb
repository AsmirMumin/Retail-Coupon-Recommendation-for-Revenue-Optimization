{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Assignment MLiM - WS 20/21\n",
    "\n",
    "Notebook contains:\n",
    "* load data set & import packages\n",
    "* lightGBM\n",
    "\n",
    "### Import Packages\n",
    "\n",
    "TO DO: \n",
    "\n",
    "* Please reassign your corresponding paths\n",
    "    * Set path where data sets are stored; newly created data sets will also be stored there\n",
    "    * Set path to where the module is stored; this path will also be used to store the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data Set\n",
    "\n",
    "**In case the data sets are already loaded, proceed with code starting at 'Train-Test-Split'**\n",
    "\n",
    "* Prerequisites: the following data frames are needed. They shoud be named:\n",
    "    * baskets.parquet (provided/downloaded)\n",
    "    * coupons.parquet (provided/downloaded)\n",
    "    * df_negative_samples.parquet (created)\n",
    "    * product_categories.csv (created)\n",
    "    * avg_no_weeks_between_two_purchases.parquet (created)\n",
    "    * lags.parquet (created)\n",
    "    * purchase_temporal_distribution.parquet (created)\n",
    "    \n",
    "* Parameters:\n",
    "    * path (where data sets are stored)\n",
    "    * train_start (first week that the training set shoud start with)\n",
    "    * test_start (analogously to train)\n",
    "    * test_end (analogously to train)\n",
    "    \n",
    "* Output:\n",
    "    * one train data set \n",
    "    * one test data set\n",
    "    * both data sets are also stored as train_s2000_final.parquet and test_s2000_final.parquet in the pwd\n",
    "    \n",
    "* Table of Content:\n",
    "    * Load Data Sets\n",
    "    * Merge Data Sets\n",
    "    * Feature Engineering Part I \n",
    "    * Unit Test Block I \n",
    "    * Train-Test-Split\n",
    "    * Clear Memory\n",
    "    * Feautre Engineering Part II.a\n",
    "    * Clear Memory\n",
    "    * Feature Engineering Part III.a\n",
    "    * Imputing/Fixing Missing Values\n",
    "    * Unit Test Block II.a\n",
    "    * Store data_train\n",
    "    * Feature Engineering Part III.b\n",
    "    * Imputing/ Fixing Missing Values\n",
    "    * Unit Test Block II.b\n",
    "    * Store data_test\n",
    "    * Unit Test Block III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Bisa/Documents/Studium/Masterstudium/5_Semester/MLiM/FinalAssignment\n"
     ]
    }
   ],
   "source": [
    "'TO DO: Please reassign your corresponding paths:'\n",
    "\n",
    "#set path where data sets are stored\n",
    "path_datasets = '/Users/Bisa/Documents/Studium/Masterstudium/5_Semester/MLiM/datasets'\n",
    "#path_datasets = '/Users/asmir/mlim_project/mlim/exercises/assignment'\n",
    "#set path to where the module is stored\n",
    "%cd '/Users/Bisa/Documents/Studium/Masterstudium/5_Semester/MLiM/FinalAssignment'\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'coupon_assignment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e81c4360514a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# coupon recommendations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcoupon_assignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcoupon_assignment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoupon_assignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'coupon_assignment'"
     ]
    }
   ],
   "source": [
    "# data wrangling \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# data structure and feature creation \n",
    "import module_generate_dataset\n",
    "from module_generate_dataset import generate_dataset\n",
    "\n",
    "# train-test-splitting\n",
    "import module_train_test_splitting\n",
    "from module_train_test_splitting import train_test_splitting\n",
    "\n",
    "# modeling \n",
    "import module_lightgbm\n",
    "from module_lightgbm import predict_lightgbm\n",
    "\n",
    "#load or train model\n",
    "import module_pretrained_train_model\n",
    "from module_pretrained_train_model import module_pretrained_train_model\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import pickle\n",
    "\n",
    "# evaluation \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# coupon recommendations \n",
    "import coupon_assignment \n",
    "from coupon_assignment import coupon_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basket generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basket code here (christopher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category code here (christopher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preferences & negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preferences & negative samples code here (christopher) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### week 90 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week 90 product sampling code here (christopher) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lagged features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lagged feature code here (christoher)\n",
    "# dependencies to featue engineering ! CAUTION!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### final data creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the module outputs two dataframes: the train and test dataset\n",
    "#generate_dataset(path, train_start, train_end, test_start, test_end)\n",
    "train, test = generate_dataset(path_datasets, 0, 88, 1, 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter are path, train_start, train_end, test_start, test_end, eval_set == True)\n",
    "\n",
    "X_train, X_test, X_eval, y_train, y_eval, y_test = train_test_splitting(path_datasets, 0, 87, 89, 89, eval_set = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters are X_train, X_test, y_train, y_test, X_eval = None, y_eval = None, eval_set = True, output_probabilities = True, n_estimators = 300, early_stopping_rounds = 50, reg_alpha = 0.3, subsample = 0.5, learning_rate = 0.01, \n",
    "#max_depth = 8, verbose = 200. All parameter that is not specified here are the default parameters of the lightgbm model\n",
    "\n",
    "y_pred = predict_lightgbm(X_train, X_test, y_train, y_test, X_eval, y_eval, eval_set = True, output_probabilities = True, n_estimators = 5, early_stopping_rounds = 50)\n",
    "\n",
    "#y_pred = a model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Week 90  (Franzi:done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import module_week90_generate_dataset\n",
    "from module_week90_generate_dataset import week90_generate_dataset\n",
    "\n",
    "#generates dataset for week 90 to make predictions based on it\n",
    "data_week90 = week90_generate_dataset(path_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coupon_assignment \n",
    "from coupon_assignment import coupon_assigmnet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load trained LGBM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path should point to the pretrained module; however, this should have been set in the beginning anyways\n",
    "#prerequisite: X_train, y_train, X_test, y_test have to be loaded\n",
    "import module_pretrained_train_model\n",
    "from module_pretrained_train_model import module_pretrained_train_model\n",
    "model = module_pretrained_train_model(pretrained_model = True, X_train = None, X_test = None, y_train = None, y_test = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'lgbm_model.pkl'\n",
    "lightgbm_model = coupon_assignment.load_model(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions and Classification Report\n",
    "lgbm_pred = lightgbm_model.predict(X_test)\n",
    "# Probabilities are returned for each class, unfortunately without the class names\n",
    "lgbm_pred_prob = lightgbm_model.predict_proba(X_test)\n",
    "lgbm_pred_prob = lgbm_pred_prob[:,1]\n",
    "\n",
    "print('Classification Accuracy: ',   metrics.accuracy_score(y_test, lgbm_pred))\n",
    "print('Classification Error: \\t',    1 - metrics.accuracy_score(y_test, lgbm_pred))\n",
    "print(\"Recall:  \\t\\t\",               metrics.recall_score(y_test, lgbm_pred))\n",
    "print(\"Precision: \\t\\t\",             metrics.precision_score(y_test, lgbm_pred))\n",
    "print(\"F1 score: \\t\\t\",              metrics.f1_score(y_test, lgbm_pred))\n",
    "print('AUC: \\t\\t\\t',                 metrics.roc_auc_score(y_test, lgbm_pred_prob))\n",
    "print('Cross Entropy: \\t\\t\\t',       metrics.log_loss(y_test, xgb_pred_prob))\n",
    "\n",
    "cm = confusion_matrix(y_test,lgbm_pred)\n",
    "ax = sns.heatmap(cm,cmap='viridis_r', annot=True, fmt='d', square=True)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_coup = coupon_assignment.coupon_assignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_coup.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>shopper</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>90</td>\n",
       "      <td>1999</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>90</td>\n",
       "      <td>1999</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>90</td>\n",
       "      <td>1999</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>90</td>\n",
       "      <td>1999</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>90</td>\n",
       "      <td>1999</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        week  shopper  product\n",
       "0         90        0        0\n",
       "1         90        0        1\n",
       "2         90        0        2\n",
       "3         90        0        3\n",
       "4         90        0        4\n",
       "...      ...      ...      ...\n",
       "499995    90     1999      245\n",
       "499996    90     1999      246\n",
       "499997    90     1999      247\n",
       "499998    90     1999      248\n",
       "499999    90     1999      249\n",
       "\n",
       "[500000 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TO DO: unit tests for final prediction\n",
    "\n",
    "#data for final prediction (week, shopper, coupon, product, discount)\n",
    "pred_coup = pd.read_parquet(path_datasets + '/prediction_index.parquet')\n",
    "pred_coup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert final_pred.shape[0] == 10000 #10.000 = 2000*5\n",
    "assert final_pred.shape[1] == 5 #shopper, week, coupon, product, discount\n",
    "assert final_pred.isna().sum().sum() == 0\n",
    "assert list(final_pred.columns) == ['shopper', 'week', 'coupon', 'product' 'discount']\n",
    "assert max(final_pred['coupon']) == 5 #did this change to 4\n",
    "assert min(final_pred['coupon']) == 1 #did this change to 0\n",
    "assert list(final_pred['discount']).unique() == [15, 20, 25, 30]\n",
    "assert final_pred['shopper'].nunique() == 2000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
