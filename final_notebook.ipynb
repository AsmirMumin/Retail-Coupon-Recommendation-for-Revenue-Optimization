{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Assignment MLiM - WS 20/21\n",
    "\n",
    "Notebook contains:\n",
    "* load data set & import packages\n",
    "* lightGBM\n",
    "\n",
    "### Import Packages\n",
    "\n",
    "TO DO: \n",
    "\n",
    "* Please reassign your corresponding paths\n",
    "    * Set path where data sets are stored; newly created data sets will also be stored there\n",
    "    * Set path to where the module is stored; this path will also be used to store the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data Set\n",
    "\n",
    "**In case the data sets are already loaded, proceed with code starting at 'Train-Test-Split'**\n",
    "\n",
    "* Prerequisites: the following data frames are needed. They shoud be named:\n",
    "    * baskets.parquet (provided/downloaded)\n",
    "    * coupons.parquet (provided/downloaded)\n",
    "    * df_negative_samples.parquet (created)\n",
    "    * product_categories.csv (created)\n",
    "    * avg_no_weeks_between_two_purchases.parquet (created)\n",
    "    * lags.parquet (created)\n",
    "    * purchase_temporal_distribution.parquet (created)\n",
    "    \n",
    "* Parameters:\n",
    "    * path (where data sets are stored)\n",
    "    * train_start (first week that the training set shoud start with)\n",
    "    * test_start (analogously to train)\n",
    "    * test_end (analogously to train)\n",
    "    \n",
    "* Output:\n",
    "    * one train data set \n",
    "    * one test data set\n",
    "    * both data sets are also stored as train_s2000_final.parquet and test_s2000_final.parquet in the pwd\n",
    "    \n",
    "* Table of Content:\n",
    "    * Load Data Sets\n",
    "    * Merge Data Sets\n",
    "    * Feature Engineering Part I \n",
    "    * Unit Test Block I \n",
    "    * Train-Test-Split\n",
    "    * Clear Memory\n",
    "    * Feautre Engineering Part II.a\n",
    "    * Clear Memory\n",
    "    * Feature Engineering Part III.a\n",
    "    * Imputing/Fixing Missing Values\n",
    "    * Unit Test Block II.a\n",
    "    * Store data_train\n",
    "    * Feature Engineering Part III.b\n",
    "    * Imputing/ Fixing Missing Values\n",
    "    * Unit Test Block II.b\n",
    "    * Store data_test\n",
    "    * Unit Test Block III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'TO DO: Please reassign your corresponding paths:'\n",
    "\n",
    "#set path where data sets are stored\n",
    "path_datasets = '/Users/Bisa/Documents/Studium/Masterstudium/5_Semester/MLiM/datasets'\n",
    "#path_datasets = '/Users/asmir/mlim_project/mlim/exercises/assignment'\n",
    "#set path to where the module is stored\n",
    "%cd '/Users/Bisa/Documents/Studium/Masterstudium/5_Semester/MLiM/FinalAssignment'\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data wrangling \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# data structure and feature creation \n",
    "import module_generate_dataset_new \n",
    "from module_generate_dataset_new import generate_dataset\n",
    "\n",
    "# train-test-splitting\n",
    "import module_train_test_splitting\n",
    "from module_train_test_splitting import train_test_splitting\n",
    "\n",
    "# modeling \n",
    "import module_lightgbm\n",
    "from module_lightgbm import lightgbm_model\n",
    "\n",
    "import lightgbm as lgbm\n",
    "import pickle\n",
    "\n",
    "# evaluation \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# coupon recommendations \n",
    "import coupon_assignment \n",
    "from coupon_assignment import coupon_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### basket generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basket code here (christopher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category code here (christopher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preferences & negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preferences & negative samples code here (christopher) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### week 90 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week 90 product sampling code here (christopher) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lagged features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lagged feature code here (christoher)\n",
    "# dependencies to featue engineering ! CAUTION!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### final data creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the module outputs two dataframes: the train and test dataset\n",
    "#generate_dataset(path, train_start, train_end, test_start, test_end)\n",
    "train, test = generate_dataset(path_datasets, 0, 88, 1, 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter are path, train_start, train_end, test_start, test_end, eval_set == True)\n",
    "\n",
    "X_train, X_test, X_eval, y_train, y_eval, y_test = train_test_splitting(path_datasets, 0, 87, 89, 89, eval_set = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters are X_train, X_test, y_train, y_test, X_eval = None, y_eval = None, eval_set = True, output_probabilities = True, n_estimators = 300, early_stopping_rounds = 50, reg_alpha = 0.3, subsample = 0.5, learning_rate = 0.01, \n",
    "#max_depth = 8, verbose = 200. All parameter that is not specified here are the default parameters of the lightgbm model\n",
    "\n",
    "y_pred = lightgbm_model(X_train, X_test, y_train, y_test, X_eval, y_eval, eval_set = True, output_probabilities = True, n_estimators = 5, early_stopping_rounds = 50)\n",
    "\n",
    "#y_pred = a model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Week 90  (Franzi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week 90 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coupon_assignment \n",
    "from coupon_assignment import coupon_assigmnet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load trained LGBM model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Franzi\n",
    "* optional parameter [load_model = True] \n",
    "    * else (False) --> train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'lgbm_model.pkl'\n",
    "lightgbm_model = coupon_assignment.load_model(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions and Classification Report\n",
    "lgbm_pred = lightgbm_model.predict(X_test)\n",
    "# Probabilities are returned for each class, unfortunately without the class names\n",
    "lgbm_pred_prob = lightgbm_model.predict_proba(X_test)\n",
    "lgbm_pred_prob = lgbm_pred_prob[:,1]\n",
    "\n",
    "print('Classification Accuracy: ',   metrics.accuracy_score(y_test, lgbm_pred))\n",
    "print('Classification Error: \\t',    1 - metrics.accuracy_score(y_test, lgbm_pred))\n",
    "print(\"Recall:  \\t\\t\",               metrics.recall_score(y_test, lgbm_pred))\n",
    "print(\"Precision: \\t\\t\",             metrics.precision_score(y_test, lgbm_pred))\n",
    "print(\"F1 score: \\t\\t\",              metrics.f1_score(y_test, lgbm_pred))\n",
    "print('AUC: \\t\\t\\t',                 metrics.roc_auc_score(y_test, lgbm_pred_prob))\n",
    "print('Cross Entropy: \\t\\t\\t',       metrics.log_loss(y_test, xgb_pred_prob))\n",
    "\n",
    "cm = confusion_matrix(y_test,lgbm_pred)\n",
    "ax = sns.heatmap(cm,cmap='viridis_r', annot=True, fmt='d', square=True)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_coup = coupon_assignment.coupon_assignment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_coup.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: unit tests for final prediction\n",
    "\n",
    "#data for final prediction (week, shopper, coupon, product, discount)\n",
    "pred_coup = pd.read_parquet(path_datasets + '/coupon_index.parquet')\n",
    "pred_coup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
